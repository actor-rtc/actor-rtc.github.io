# 专题解析：生产环境就绪 (Production Readiness)

将一个基于本框架的应用从原型阶段推向生产环境，需要在一系列关键领域进行加固，以确保系统的可靠性、可观测性和可扩展性。本文档提供了在生产环境中部署、监控和维护 Actr 应用的最佳实践和核心考量。

## 1. 配置管理 (Configuration Management)

生产环境的配置应与开发环境分离，并支持灵活、安全的管理。

*   **外部化配置**: 避免将配置硬编码在代码中。推荐使用环境变量或独立的配置文件（如 `config.toml`）来管理 `ActrSystem` 的参数，例如线程池大小、队列容量、日志级别等。
*   **敏感信息管理**: 对于 TURN 服务器的凭证、数据库密码等敏感信息，**严禁**将其提交到代码仓库。应使用专门的密钥管理服务 (KMS)，或通过安全的环境变量在部署时注入。
*   **网络配置**: 确保为 `ActrSystem` 配置了 STUN 和 TURN 服务器的地址和凭证。在真实的公网环境中，没有 TURN 服务器将导致大部分跨 NAT 的连接失败。

## 2. 日志记录 (Logging)

日志是生产环境问题排查的生命线。

*   **日志级别**: 在生产环境中，默认的日志级别应设置为 `INFO`。`DEBUG` 和 `TRACE` 级别会产生大量日志，对性能有显著影响，只应在排查特定问题时动态开启。
*   **结构化日志**: 强烈推荐使用**结构化日志**（如 JSON 格式）。相比于纯文本日志，JSON 格式的日志可以被日志聚合系统（如 ELK, Datadog, Splunk）轻松地索引和查询。
*   **分布式追踪**: 系统使用 OpenTelemetry 和 W3C Trace Context 标准进行分布式追踪。日志会自动关联到当前的追踪 span，无需手动提取 trace_id。

    *示例 (伪代码)*:
    ```rust
    // 在 Actor 方法中
    // 使用 tracing 宏，span 信息会自动包含在日志中
    tracing::info!(
        message = "User profile updated",
        user_id = profile.user_id
    );
    // span 信息（trace_id, span_id）会自动添加到日志记录中
    ```

## 3. 监控与指标 (Monitoring & Metrics)

没有监控的系统就是“黑盒”。你需要一套机制来度量系统的健康状况和性能。

*   **暴露指标**: 框架应提供一种方式来暴露内部的关键性能指标。最标准的方式是提供一个 `/metrics` HTTP 端点，以 **Prometheus 格式**输出指标。
*   **关键指标**: 以下是必须监控的核心指标：
    *   **系统指标**: 进程的 CPU 和内存使用率。
    *   **ActrSystem 指标**:
        *   `actorsystem_active_threads`: 异步运行时中活跃线程的数量。
        *   `actorsystem_webrtc_connections_total`: 当前活跃的 WebRTC 连接总数。
        *   `actorsystem_ice_negotiation_failures_total`: ICE 协商失败的累计次数。
    *   **Actor 指标** (按 Actor 类型/ID 区分):
        *   `actor_mailbox_queue_length`: 邮箱中的消息队列长度。如果此值持续增长，说明 Actor 已成为性能瓶颈。
        *   `actor_message_processing_duration_seconds`: 处理单个消息的耗时分布（直方图）。有助于定位慢消息。
    *   **WebRTC 媒体质量指标** (按 Track 区分):
        *   `webrtc_track_packet_loss_rate`: 丢包率。
        *   `webrtc_track_jitter_seconds`: 抖动。
        *   `webrtc_track_round_trip_time_seconds`: 往返延迟 (RTT)。

## 4. 错误处理与韧性 (Error Handling & Resilience)

生产系统必须能够优雅地处理失败。Actor-RTC 采用**微服务视角**的错误处理哲学，区别于经典 Actor 系统（如 Erlang/Akka）的 Supervision 模式。

### 4.1 设计哲学：职责分离

**核心原则**："框架负责传输，应用负责业务"

| 错误类型 | 处理责任 | 处理方式 |
|---------|---------|---------|
| **应用错误** | 应用代码 | 作为 RPC 响应返回给调用方 |
| **框架错误** | 框架层 | 分类处理（重试/死信队列/返回错误） |

**与经典 Actor 系统的对比**：

| 维度 | 经典 Actor (Erlang/Akka) | Actor-RTC (微服务视角) |
|------|-------------------------|----------------------|
| **错误处理主体** | Supervisor（框架） | **调用方（应用）** |
| **重试责任** | 框架自动重启 Actor | **调用方决定是否重试** |
| **透明性** | 透明重启，调用方无感知 | **显式错误返回，调用方处理** |
| **错误传播** | 隔离在 Actor 内部 | **作为 RPC 响应传播** |

**设计理由**：每个 Actr 进程是宏观的"微服务单元"而非微观的"轻量级 Actor"，应遵循微服务的错误处理最佳实践。

---

### 4.2 应用错误处理

**定义**：Workload handler 处理消息时返回的业务层错误（如 `handle_echo()` 返回 `Err(MyBusinessError)`）。

**框架行为**：
1. 将错误序列化为 Protobuf
2. 作为正常的 RPC 响应返回给调用方
3. 调用方的 `ctx.call()` 收到此错误

**示例**：
```rust
// 目标 Actor (被调用方)
impl EchoHandler for MyWorkload {
    async fn handle_echo<C: Context>(
        &self,
        request: EchoRequest,
        ctx: &C,
    ) -> Result<EchoResponse, MyBusinessError> {
        if request.message.is_empty() {
            // 应用错误：参数校验失败
            return Err(MyBusinessError::EmptyMessage);
        }
        Ok(EchoResponse { ... })
    }
}

// 调用方 Actor
match ctx.call(&target, EchoRequest { message: "" }).await {
    Ok(response) => { /* 处理成功响应 */ }
    Err(e) => {
        // 收到应用错误，由调用方决定如何处理
        tracing::warn!("Business error: {:?}", e);
        // 可以选择重试、降级、返回错误给上游等
    }
}
```

**关键点**：
- ✅ 应用错误是"返回值的一部分"，不是异常情况
- ✅ 调用方完全感知错误，自行决定处理策略
- ❌ 框架**不**自动重试应用错误
- ❌ 框架**不**将应用错误记录到死信队列

---

### 4.3 框架错误分类与处理

**定义**：框架层面的传输、编解码、连接管理等错误。

#### 4.3.1 错误分类体系

**Transient（瞬态错误）** - 临时性故障，可重试：
- `UNAVAILABLE` - 目标 Actor 暂时不可达（连接断开、过载）
- `DEADLINE_EXCEEDED` - 请求超时
- 网络抖动、临时资源不足

**Permanent（永久错误）** - 需要修复系统状态，不可重试：
- `NOT_FOUND` - 目标 Actor 不存在（需先 discovery）
- `INVALID_ARGUMENT` - 消息格式错误（需修复调用代码）
- `FAILED_PRECONDITION` - 前置条件未满足

**Poison Message（毒消息）** - 无法处理的消息，需人工介入：
- Protobuf 解码失败（消息损坏）
- 重复处理多次仍失败

#### 4.3.2 具体场景处理策略

**场景 A：Mailbox Dequeue 失败**（数据库损坏）
- **处理**：返回 `INTERNAL` 错误 + 指标告警
- **调用方**：可选择重试或降级
- **监控**：触发告警，人工介入修复数据库

**场景 B：Protobuf 解码失败**（消息损坏）
- **处理**：Dead Letter Queue + 结构化日志
- **日志内容**：
  ```rust
  tracing::error!(
      raw_bytes = ?bytes,
      error = %e,
      "Protobuf decode failed, moved to DLQ"
  );
  // trace_id 和 span_id 会自动添加到日志中
  ```
- **后续**：人工分析 DLQ，修复消息格式后 redrive

**场景 C：目标 Actor 不存在**
- **处理**：返回 `NOT_FOUND` 错误（永久错误）
- **调用方**：不应重试，需先通过 discovery 确认目标存在
- **示例**：
  ```rust
  Err(RuntimeError::NotFound {
      actor_id: target.clone(),
      message: "Target actor not registered in signaling server",
  })
  ```

**场景 D：WebRTC 连接断开**
- **处理**：
  1. 框架自动重连（已实现智能重连机制）
  2. 返回 `UNAVAILABLE` 错误（瞬态错误）
- **调用方**：可使用指数退避重试
- **用户体验**：显示"正在重新连接..."提示

**场景 E：Handler Panic**（Workload 代码崩溃）
- **处理**：
  1. 捕获 panic（使用 `std::panic::catch_unwind`）
  2. 返回 `INTERNAL` 错误 + 详细日志
  ```rust
  tracing::error!(
      request_id = %request_id,
      panic_message = %panic_info,
      "Handler panicked during message processing"
  );
  // trace_id 和 span_id 会自动添加到日志中
  ```
- **不重启 Actor**：微服务不会因为一个请求失败就重启整个服务
- **调用方**：收到 `INTERNAL` 错误，可选择降级或返回错误给上游

**场景 F：Handler 超时**（执行时间过长）
- **处理**：
  1. 返回 `DEADLINE_EXCEEDED` 错误
  2. 取消后台任务（使用 `tokio::time::timeout`）
- **配置**：可配置超时时间（默认建议 30 秒）
- **调用方**：决定是否重试或降级

---

### 4.4 Dead Letter Queue (DLQ)

**用途**：存储无法处理的毒消息，供人工分析和重新处理。

**触发条件**：
1. Protobuf 解码失败
2. 超过最大重试次数（如 3 次）仍失败的框架错误

**实现**（推荐）：
```rust
// DLQ 表结构（SQLite）
CREATE TABLE dead_letter_queue (
    id INTEGER PRIMARY KEY,
    trace_id TEXT NOT NULL,  -- 从 W3C traceparent 头提取的追踪 ID
    request_id TEXT NOT NULL,
    caller_id TEXT,
    target_id TEXT NOT NULL,
    payload_type INTEGER NOT NULL,
    raw_bytes BLOB NOT NULL,
    error_message TEXT NOT NULL,
    retry_count INTEGER DEFAULT 0,
    created_at INTEGER NOT NULL,
    last_retry_at INTEGER
);
```

**工作流**：
```
消息 → 处理失败 → 重试 3 次 → 仍失败 → DLQ
                                    ↓
                          人工分析 → 修复问题 → Redrive API 回主队列
```

**Redrive API**：
```rust
// 将 DLQ 消息重新发送到主队列
mailbox.redrive_from_dlq(message_id).await?;
```

---

### 4.5 Retry Pattern（重试模式）

**框架提供**：辅助函数，不强制使用

```rust
// 示例：辅助函数（未来实现）
use actr_framework::retry::{retry_with_backoff, ExponentialBackoff};

let result = retry_with_backoff(
    || ctx.call(&target, request.clone()),
    ExponentialBackoff {
        max_retries: 3,
        initial_delay: Duration::from_secs(1),
        max_delay: Duration::from_secs(10),
        multiplier: 2.0,
    },
    |error| {
        // 只重试瞬态错误
        matches!(error, RuntimeError::Unavailable | RuntimeError::DeadlineExceeded)
    }
).await?;
```

**指数退避时序**：
```
Retry 1: 1s 后
Retry 2: 2s 后
Retry 3: 4s 后
Retry 4: 8s 后（如果 max_retries > 3）
```

**调用方完全控制**：
- 应用自己决定哪些错误可重试
- 应用自己决定重试策略（次数、延迟）
- 应用自己决定是否使用 Circuit Breaker

---

### 4.6 Circuit Breaker（熔断器）

**三种状态**：
- **Closed**（闭合）：正常通过请求
- **Open**（断开）：直接失败，不发送请求（快速失败）
- **Half-Open**（半开）：尝试少量请求测试恢复

**作用**：
1. 防止级联故障
2. 给下游 Actor 恢复时间
3. 快速失败，不浪费资源

**实现示例**（推荐使用第三方库如 `failsafe-rs`）：
```rust
use failsafe::{CircuitBreaker, Config};

let circuit_breaker = CircuitBreaker::new(
    Config::new()
        .failure_rate_threshold(50.0)  // 50% 失败率触发
        .minimum_number_of_calls(10)   // 至少 10 次调用
        .wait_duration_in_open_state(Duration::from_secs(30))
);

match circuit_breaker.call(|| ctx.call(&target, request.clone())).await {
    Ok(response) => { /* 处理成功 */ }
    Err(e) if e.is_circuit_open() => {
        // Circuit Breaker 已打开，快速失败
        tracing::warn!("Circuit breaker open for target: {:?}", target);
        // 降级处理或返回缓存结果
    }
    Err(e) => { /* 其他错误 */ }
}
```

**与 Retry 的配合**：
- Retry 检测到 Circuit Breaker Open 时应**立即停止重试**
- Circuit Breaker 优先级高于 Retry

---

### 4.7 可观测性要求

**Correlation ID / Trace ID**：

所有错误必须携带完整的追踪信息：
```rust
tracing::error!(
    request_id = %ctx.request_id(),
    caller_id = ?ctx.caller_id(),
    target_id = %target.serial_number,
    error_type = "UNAVAILABLE",
    retry_count = %retry_count,
    "RPC call failed after retries"
);
// trace_id 和 span_id 会自动添加到日志中（通过当前的 tracing span）
```

**关键指标**：
- `rpc_calls_total{status="success|error", error_type="..."}` - RPC 调用总数
- `rpc_call_duration_seconds{quantile="0.5|0.95|0.99"}` - RPC 延迟分位数
- `rpc_retry_count` - 重试次数分布
- `circuit_breaker_state{target="..."}` - 熔断器状态
- `dlq_message_count` - 死信队列消息数量

**分布式追踪**：
- 使用 OpenTelemetry 标准
- Trace ID 通过 gRPC Metadata / HTTP Header 传播
- 跨 Actor 调用链完整可追溯

---

### 4.8 最佳实践总结

**✅ 推荐做法**：

1. **应用错误透传**：不要在框架层捕获应用错误，直接返回给调用方
2. **显式错误分类**：清晰区分 Transient vs Permanent 错误
3. **调用方控制重试**：提供工具函数，不强制策略
4. **结构化日志**：所有错误必须带 request_id、caller_id（trace_id 由 span 自动添加）
5. **指标驱动告警**：监控错误率、延迟、重试次数，设置阈值告警
6. **DLQ 人工介入**：定期检查 DLQ，修复毒消息

**❌ 避免做法**：

1. ❌ 框架自动重试应用错误（违背"应用错误由应用处理"原则）
2. ❌ 无限重试（必须设置 max_retries）
3. ❌ 忽略错误分类，对所有错误统一重试
4. ❌ 在日志中丢失追踪信息（无法追踪调用链）- 现在使用 OpenTelemetry 自动处理
5. ❌ Actor panic 导致进程崩溃（必须捕获 panic）

---

**参考资料**：
- 《[1 理念与架构](./1-concepts-and-architecture.zh.md)》- 微服务视角补充
- 《[4.6 actr-runtime](./4.6-actr-runtime.zh.md)》- 错误类型定义
- gRPC Status Codes: https://grpc.io/docs/guides/status-codes/
- AWS DLQ Best Practices: https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/

## 5. 部署与扩展 (Deployment & Scalability)

## 5. 部署与扩展 (Deployment & Scalability)

*   **依赖服务**: 
    *   **信令服务器**: 
        > [!WARNING]
        > **信令服务器是系统的核心单点故障（SPOF）**
        >
        > 一个单实例的信令服务器仅适用于开发和测试。在生产环境中，**必须**部署高可用的信令服务器集群，否则其一旦故障将导致整个 `actr` 网络的控制平面瘫痪。

        对于大规模部署，必须将信令服务器设计为可水平扩展的无状态服务，并使用 Redis、Nats 或 Etcd 等外部高可用组件作为共享后端，以同步所有 Actor 的在线状态和路由信息。

    *   **TURN 服务器**: 部署高可用的开源 TURN 服务器（如 `coturn`）是保证连接成功率的关键。建议在全球不同地理位置部署多个 TURN 节点，并使用负载均衡。
*   **容器化**: 将你的 Actor 服务打包成 Docker 镜像，并使用 Kubernetes 或类似的容器编排平台进行部署，可以极大地简化管理、伸缩和滚动更新的复杂度。
*   **状态管理**: 如果你的 Actor 是无状态的（即所有状态都持久化在外部数据库中），那么你可以简单地通过启动多个服务实例来进行水平扩展。如果你的 Actor 是有状态的（即将状态保存在内存中），水平扩展将变得非常复杂，此时应优先考虑垂直扩展（增加单个实例的 CPU 和内存），或在业务上进行拆分。
